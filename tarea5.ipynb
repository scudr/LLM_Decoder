{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Carga del Modelo Decoder Preentrenado\n",
    "\n",
    "Cargamos el modelo GPT-2 Large, que es un modelo preentrenado de generaci√≥n de texto. Este modelo es adecuado para tareas donde se necesita generar respuestas coherentes y contextualmente adecuadas a partir de una entrada textual.\n",
    "\n",
    "Los modelos GPT-2 est√°n disponibles en varios tama√±os . \n",
    "- gpt2: 110M par√°metros\n",
    "- gpt2-medium: 345M par√°metros\n",
    "- gpt2-large: 774M par√°metros\n",
    "- gpt2-xl: 1558M par√°metros\n",
    "\n",
    " Usamos la variante 'gpt2-large' para aprovechar su capacidad de generalizaci√≥n en textos largos y complejos. Descartamos de usar el gpt2-xl por el gran tama√±o de  par√°metro y recursos necesarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptaci√≥n del c√≥digo del proyecto para utilizar el modelo decoder en tareas de generaci√≥n de texto \n",
    "El prop√≥sito de esta secci√≥n es demostrar c√≥mo utilizar el modelo GPT-2 para la generaci√≥n de texto. Se implementa una funci√≥n que toma un texto de entrada, lo tokeniza y genera una secuencia de texto que sigue el contexto proporcionado. Este c√≥digo es fundamental para adaptar el modelo preentrenado a tareas espec√≠ficas de generaci√≥n de texto, como respuestas autom√°ticas o completaci√≥n de oraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a remote place, a man named John Smith was walking along the shore of a lake when he saw a strange creature. The creature was a large, black, hairy creature with a long, thin tail. The creature was about\n"
     ]
    }
   ],
   "source": [
    "# Asignar eos_token como pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_text(input_text: str, model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, max_length: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Genera texto basado en la entrada dada utilizando un modelo de lenguaje preentrenado.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): El texto de entrada sobre el cual se generar√° la continuaci√≥n.\n",
    "        model (GPT2LMHeadModel): El modelo seleccionado entrenado para generaci√≥n de texto.\n",
    "        tokenizer (GPT2Tokenizer): El tokenizador\n",
    "        max_length (int, optional): La longitud m√°xima de la secuencia generada. Por defecto es 50.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto generado por el modelo.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Tokenizar la entrada y generar la m√°scara de atenci√≥n\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Generar el texto\n",
    "    output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(generate_text(\"Once upon a time in a remote place\", model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "Para adaptar el modelo GPT-2 Large a un dominio espec√≠fico, realizamos un proceso de fine-tuning utilizando el dataset PubMedQA. Este dataset contiene preguntas y respuestas del √°mbito m√©dico, lo cual es ideal para entrenar el modelo a generar respuestas coherentes y contextualmente precisas en temas de medicina. Esto permite mejorar la precisi√≥n del modelo en la generaci√≥n de texto para aplicaciones espec√≠ficas en este campo.\n",
    "\n",
    "**Diccionario de datos del dataset de PubMedQA**\n",
    "- pubid: Un identificador √∫nico para cada pregunta.\n",
    "- question: La pregunta que se hace sobre un tema m√©dico.\n",
    "- context: Informaci√≥n contextual relacionada con la pregunta.\n",
    "- long_answer: La respuesta extensa a la pregunta.\n",
    "- final_decision: Un resumen o decisi√≥n final, que podr√≠a ser una simple respuesta afirmativa o negativa (\"yes\" o \"no\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pubid                                           question  \\\n",
      "0  21645374  Do mitochondria play a role in remodelling lac...   \n",
      "1  16418930  Landolt C and snellen e acuity: differences in...   \n",
      "2   9488747  Syncope during bathing in infants, a pediatric...   \n",
      "3  17208539  Are the long-term results of the transanal pul...   \n",
      "4  10808977  Can tailored interventions increase mammograph...   \n",
      "\n",
      "                                             context  \\\n",
      "0  {'contexts': ['Programmed cell death (PCD) is ...   \n",
      "1  {'contexts': ['Assessment of visual acuity dep...   \n",
      "2  {'contexts': ['Apparent life-threatening event...   \n",
      "3  {'contexts': ['The transanal endorectal pull-t...   \n",
      "4  {'contexts': ['Telephone counseling and tailor...   \n",
      "\n",
      "                                         long_answer final_decision  \n",
      "0  Results depicted mitochondrial dynamics in viv...            yes  \n",
      "1  Using the charts described, there was only a s...             no  \n",
      "2  \"Aquagenic maladies\" could be a pediatric form...            yes  \n",
      "3  Our long-term study showed significantly bette...             no  \n",
      "4  The effects of the intervention were most pron...            yes  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   pubid           1000 non-null   int32 \n",
      " 1   question        1000 non-null   object\n",
      " 2   context         1000 non-null   object\n",
      " 3   long_answer     1000 non-null   object\n",
      " 4   final_decision  1000 non-null   object\n",
      "dtypes: int32(1), object(4)\n",
      "memory usage: 35.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer archivo parquet con pandas\n",
    "df_labeled = pd.read_parquet('PubMedQA/pqa_labeled/train-00000-of-00001.parquet')\n",
    "\n",
    "# Mostrar las primeras filas del dataframe para ver su estructura\n",
    "print(df_labeled.head())\n",
    "\n",
    "# Mostrar informaci√≥n sobre el dataframe para ver las columnas y tipos de datos\n",
    "print(df_labeled.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento del dataset PubMedQA\n",
    "Se utiliza el dataset PubMedQA, que es un repositorio de preguntas y respuestas en el √°rea de medicina. El preprocesamiento es crucial para asegurar que los datos de entrada sean consistentes y est√©n en el formato adecuado para el modelo. En este caso, se combinan las preguntas y contextos para formar una sola secuencia de entrada que el modelo utilizar√° durante el entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  Do mitochondria play a role in remodelling lac...   \n",
      "1  Landolt C and snellen e acuity: differences in...   \n",
      "2  Syncope during bathing in infants, a pediatric...   \n",
      "3  Are the long-term results of the transanal pul...   \n",
      "4  Can tailored interventions increase mammograph...   \n",
      "\n",
      "                                    summarized_input  \\\n",
      "0  Programmed cell death (PCD) is the regulated d...   \n",
      "1  100 patients (age 8 - 90 years, median 60.5 ye...   \n",
      "2  Eight infants aged 2 to 15 months were admitte...   \n",
      "3  The transanal endorectal pull-through (TERPT) ...   \n",
      "4  Can tailored interventions increase mammograph...   \n",
      "\n",
      "                                          input_text  \n",
      "0  [QUESTION] Do mitochondria play a role in remo...  \n",
      "1  [QUESTION] Landolt C and snellen e acuity: dif...  \n",
      "2  [QUESTION] Syncope during bathing in infants, ...  \n",
      "3  [QUESTION] Are the long-term results of the tr...  \n",
      "4  [QUESTION] Can tailored interventions increase...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Leer archivo parquet con pandas (si es necesario)\n",
    "# df_labeled = pd.read_parquet('PubMedQA/pqa_labeled/train-00000-of-00001.parquet')\n",
    "\n",
    "# Funci√≥n para concatenar 'question' y 'context'\n",
    "def create_input_text(row):\n",
    "    question = \"[QUESTION] \" + row['question']\n",
    "    context = \"[CONTEXT] \" + \" \".join(row['context']['contexts'])\n",
    "    return question + \" \" + context\n",
    "\n",
    "# Crear la nueva columna 'input_text' combinando 'question' y 'context'\n",
    "df_labeled['input_text'] = df_labeled.apply(create_input_text, axis=1)\n",
    "\n",
    "# Pipeline para resumir texto en GPU\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "\n",
    "def summarize_input_text(text):\n",
    "    input_length = len(text.split())\n",
    "    # Ajustar max_length a un 70% de la longitud del input, pero no mayor de 150 y no menor de 20\n",
    "    max_len = min(150, max(20, int(input_length * 0.7)))\n",
    "    summarized = summarizer(text, max_length=max_len, min_length=10, do_sample=False)\n",
    "    return summarized[0]['summary_text']\n",
    "\n",
    "# Resumir la columna 'input_text'\n",
    "df_labeled['summarized_input'] = df_labeled['input_text'].apply(summarize_input_text)\n",
    "\n",
    "# Asumiendo que ya has resumido y generado la columna 'summarized_input'\n",
    "inputs = df_labeled['summarized_input'].tolist()\n",
    "outputs = df_labeled['long_answer'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar algunas de las filas generadas para verificar los resultados\n",
    "print(df_labeled[['question', 'summarized_input', 'input_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n del Modelo  GPT2-Large  antes del Ajuste\n",
    "Antes de realizar el fine-tuning, es fundamental evaluar el rendimiento del modelo en su estado preentrenado. Esto nos proporciona una l√≠nea base para comparar los resultados posteriores al ajuste. Se emplean m√©tricas como ROUGE y METEOR para medir la calidad de las respuestas generadas, lo cual es importante para tareas de generaci√≥n de texto donde la coherencia y la precisi√≥n son clave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cajue\\AppData\\Local\\Temp\\ipykernel_1644\\451330542.py:43: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cajue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cajue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score before fine-tuning: {'rouge1': AggregateScore(low=Score(precision=0.18518391411503307, recall=0.3636803231088654, fmeasure=0.2371552833441397), mid=Score(precision=0.19990472388608943, recall=0.3859865089640316, fmeasure=0.2512090784722976), high=Score(precision=0.21530770974832683, recall=0.4098183511462053, fmeasure=0.2659871328262393)), 'rouge2': AggregateScore(low=Score(precision=0.04119553564750165, recall=0.0845137193138282, fmeasure=0.0529199165039611), mid=Score(precision=0.04739487692077794, recall=0.09934890898686592, fmeasure=0.060810566475866835), high=Score(precision=0.054243848071813185, recall=0.11533246988791568, fmeasure=0.0686940471216869)), 'rougeL': AggregateScore(low=Score(precision=0.11643986027572278, recall=0.23400278652838252, fmeasure=0.15008412720426026), mid=Score(precision=0.12551745050848523, recall=0.252621044122328, fmeasure=0.159853160510098), high=Score(precision=0.13412005987519693, recall=0.2732104508013579, fmeasure=0.16822604413380235)), 'rougeLsum': AggregateScore(low=Score(precision=0.12149276204620559, recall=0.24117818630920648, fmeasure=0.15607002943707649), mid=Score(precision=0.13036289579744448, recall=0.2618507131306208, fmeasure=0.16558104967245807), high=Score(precision=0.14045201905735083, recall=0.2820937434717459, fmeasure=0.1762352343442939))}\n",
      "METEOR Score before fine-tuning: {'meteor': 0.24693342198199728}\n",
      "ROUGE Score after fine-tuning: {'rouge1': AggregateScore(low=Score(precision=0.18518391411503307, recall=0.3636803231088654, fmeasure=0.2371552833441397), mid=Score(precision=0.19990472388608943, recall=0.3859865089640316, fmeasure=0.2512090784722976), high=Score(precision=0.21530770974832683, recall=0.4098183511462053, fmeasure=0.2659871328262393)), 'rouge2': AggregateScore(low=Score(precision=0.04119553564750165, recall=0.0845137193138282, fmeasure=0.0529199165039611), mid=Score(precision=0.04739487692077794, recall=0.09934890898686592, fmeasure=0.060810566475866835), high=Score(precision=0.054243848071813185, recall=0.11533246988791568, fmeasure=0.0686940471216869)), 'rougeL': AggregateScore(low=Score(precision=0.11643986027572278, recall=0.23400278652838252, fmeasure=0.15008412720426026), mid=Score(precision=0.12551745050848523, recall=0.252621044122328, fmeasure=0.159853160510098), high=Score(precision=0.13412005987519693, recall=0.2732104508013579, fmeasure=0.16822604413380235)), 'rougeLsum': AggregateScore(low=Score(precision=0.12149276204620559, recall=0.24117818630920648, fmeasure=0.15607002943707649), mid=Score(precision=0.13036289579744448, recall=0.2618507131306208, fmeasure=0.16558104967245807), high=Score(precision=0.14045201905735083, recall=0.2820937434717459, fmeasure=0.1762352343442939))}\n",
      "METEOR Score after fine-tuning: {'meteor': 0.24693342198199728}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_metric\n",
    "\n",
    "# Verificar si hay una GPU disponible y mover el modelo a la GPU si es posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Inicializar el tokenizer con padding en el lado izquierdo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\", padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizar los inputs con attention_mask\n",
    "tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "tokenized_outputs = tokenizer(outputs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Calcular la longitud m√°xima de entrada\n",
    "max_input_length = max([len(input_id) for input_id in tokenized_inputs['input_ids']])\n",
    "\n",
    "# Cargar el modelo preentrenado y moverlo a la GPU\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n",
    "\n",
    "# Evaluar con un conjunto de validaci√≥n\n",
    "inputs_eval = tokenized_inputs['input_ids'][:100].to(device)\n",
    "attention_mask_eval = tokenized_inputs['attention_mask'][:100].to(device)\n",
    "outputs_eval = tokenized_outputs['input_ids'][:100].to(device)\n",
    "\n",
    "# Generaci√≥n de texto con ajuste en max_length o max_new_tokens\n",
    "generated_outputs = model.generate(\n",
    "    inputs_eval, \n",
    "    attention_mask=attention_mask_eval,\n",
    "    max_length=max_input_length + 50,  # Utiliza la longitud m√°xima de entrada calculada\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Convertir las salidas a texto para la evaluaci√≥n\n",
    "pred_texts_before = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs]\n",
    "ref_texts = [tokenizer.decode(ref, skip_special_tokens=True).strip().lower() for ref in outputs_eval]\n",
    "\n",
    "# Calcular ROUGE y METEOR antes del ajuste\n",
    "from datasets import load_metric\n",
    "\n",
    "# Calcular ROUGE y METEOR antes del ajuste\n",
    "rouge = load_metric(\"rouge\")\n",
    "meteor = load_metric(\"meteor\", trust_remote_code=True)\n",
    "\n",
    "# Convertir las salidas a texto para la evaluaci√≥n\n",
    "pred_texts_before = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs]\n",
    "ref_texts = [tokenizer.decode(ref, skip_special_tokens=True).strip().lower() for ref in outputs_eval]\n",
    "\n",
    "# Evaluar ROUGE y METEOR antes del ajuste\n",
    "rouge_score_before = rouge.compute(predictions=pred_texts_before, references=ref_texts)\n",
    "meteor_score_before = meteor.compute(predictions=pred_texts_before, references=ref_texts)\n",
    "\n",
    "print(f\"ROUGE Score before fine-tuning: {rouge_score_before}\")\n",
    "print(f\"METEOR Score before fine-tuning: {meteor_score_before}\")\n",
    "\n",
    "\n",
    "# Generaci√≥n de texto con el modelo afinado\n",
    "generated_outputs_finetuned = model.generate(\n",
    "    inputs_eval, \n",
    "    attention_mask=attention_mask_eval,\n",
    "    max_length=max_input_length + 50,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Convertir las salidas a texto para la evaluaci√≥n despu√©s del ajuste\n",
    "pred_texts_after = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs_finetuned]\n",
    "\n",
    "# Evaluar ROUGE y METEOR despu√©s del ajuste\n",
    "rouge_score_after = rouge.compute(predictions=pred_texts_after, references=ref_texts)\n",
    "meteor_score_after = meteor.compute(predictions=pred_texts_after, references=ref_texts)\n",
    "\n",
    "print(f\"ROUGE Score after fine-tuning: {rouge_score_after}\")\n",
    "print(f\"METEOR Score after fine-tuning: {meteor_score_after}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning del modelo GPT-2 Large con el dataset de PubMedQA\n",
    "El fine-tuning implica entrenar el modelo preentrenado con un conjunto de datos espec√≠fico para especializarlo en una tarea particular. En este caso, el modelo se entrena con el dataset PubMedQA para mejorar su capacidad de generar respuestas relacionadas con temas m√©dicos. Este proceso de ajuste es esencial para adaptar modelos generales a aplicaciones espec√≠ficas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 201/750 [00:32<01:29,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2986, 'grad_norm': 1.7591943740844727, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 401/750 [01:05<00:57,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9427, 'grad_norm': 2.4787657260894775, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 601/750 [01:38<00:25,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6776, 'grad_norm': 1.9071123600006104, 'learning_rate': 1e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [02:29<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 149.5856, 'train_samples_per_second': 20.055, 'train_steps_per_second': 5.014, 'train_loss': 2.8729229736328126, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Verificar si hay una GPU disponible y mover el modelo a la GPU si es posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cargar el tokenizer y el modelo preentrenado GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "# Asignar el token de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n",
    "\n",
    "# Definir una clase Dataset para los datos de entrenamiento\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # No necesitamos pin_memory aqu√≠ porque ya estamos en GPU\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenizar los inputs con attention_mask\n",
    "tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "tokenized_outputs = tokenizer(outputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# Preparar el dataset para el entrenamiento\n",
    "train_dataset = TextDataset(tokenized_inputs, tokenized_outputs['input_ids'])\n",
    "\n",
    "# Configurar los argumentos del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           # Carpeta de salida\n",
    "    overwrite_output_dir=True,        # Sobrescribir los resultados existentes\n",
    "    num_train_epochs=3,               # N√∫mero de √©pocas de entrenamiento\n",
    "    per_device_train_batch_size=4,    # Tama√±o del batch de entrenamiento\n",
    "    save_steps=10_000,                # Guardar cada 10,000 pasos\n",
    "    save_total_limit=2,               # Limitar el n√∫mero de checkpoints guardados\n",
    "    logging_dir='./logs',             # Directorio de los logs\n",
    "    logging_steps=200,                # Registrar cada 200 pasos\n",
    "    report_to=\"none\",                 # Desactivar reportes autom√°ticos (e.g., a WandB)\n",
    ")\n",
    "\n",
    "# Inicializar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Comenzar el entrenamiento (fine-tuning)\n",
    "trainer.train()\n",
    "\n",
    "# Guardar el modelo afinado\n",
    "trainer.save_model(\"./fine_tuned_gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion del modelo despues del Fine-Tuning\n",
    "Despu√©s de realizar el fine-tuning, es crucial evaluar el rendimiento del modelo para asegurar que ha habido una mejora significativa. En esta secci√≥n, se utiliza el modelo ajustado para generar texto y se calculan las m√©tricas ROUGE y METEOR para comparar los resultados antes y despu√©s del ajuste. Esta comparaci√≥n permite cuantificar la mejora en la calidad de las respuestas generadas por el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score after fine-tuning: {'rouge1': AggregateScore(low=Score(precision=0.24786907670163041, recall=0.29191895230326764, fmeasure=0.25825355272835365), mid=Score(precision=0.267293526192736, recall=0.3149955497618294, fmeasure=0.2750576249441764), high=Score(precision=0.28615510480200157, recall=0.34197500281173754, fmeasure=0.2916937529305495)), 'rouge2': AggregateScore(low=Score(precision=0.05459946486472085, recall=0.06656023866251329, fmeasure=0.057800817628249276), mid=Score(precision=0.06518164797420237, recall=0.08144907164844684, fmeasure=0.06878521917562877), high=Score(precision=0.07660377025133548, recall=0.09901924636065118, fmeasure=0.0808814355654613)), 'rougeL': AggregateScore(low=Score(precision=0.15657106900324452, recall=0.187130436255338, fmeasure=0.16355496195226907), mid=Score(precision=0.1681480779597721, recall=0.20527268052793685, fmeasure=0.17524529598808058), high=Score(precision=0.18082641258680598, recall=0.22790800619432267, fmeasure=0.18764927890193514)), 'rougeLsum': AggregateScore(low=Score(precision=0.155982782152804, recall=0.18602095544912767, fmeasure=0.16375228156731203), mid=Score(precision=0.16846679942276982, recall=0.20586841101462894, fmeasure=0.1752079387821442), high=Score(precision=0.18100181004741728, recall=0.2267962154034395, fmeasure=0.18789082619067954))}\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo afinado\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\").to(device)\n",
    "\n",
    "# Generaci√≥n de texto con el modelo afinado\n",
    "generated_outputs_finetuned = model.generate(\n",
    "    inputs_eval, \n",
    "    attention_mask=attention_mask_eval,\n",
    "    max_length=max_input_length + 50,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Reemplazar el c√°lculo de BLEU score con el c√°lculo de ROUGE score\n",
    "from datasets import load_metric\n",
    "\n",
    "# Calcular ROUGE Score despu√©s del fine-tuning, permitiendo c√≥digo remoto\n",
    "rouge = load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "# Convertir las salidas a texto para la evaluaci√≥n\n",
    "pred_texts = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs_finetuned]\n",
    "ref_texts = [tokenizer.decode(ref, skip_special_tokens=True).strip().lower() for ref in outputs_eval]\n",
    "\n",
    "# Calcular el ROUGE score\n",
    "rouge_scores = rouge.compute(predictions=pred_texts, references=ref_texts)\n",
    "\n",
    "print(f\"ROUGE Score after fine-tuning: {rouge_scores}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n del Modelo Antes y Despu√©s del Fine-Tuning\n",
    "\n",
    "### Resultados Antes del Fine-Tuning\n",
    "\n",
    "Antes de realizar el fine-tuning, los resultados del modelo GPT-2 Large en las m√©tricas ROUGE y METEOR fueron los siguientes:\n",
    "\n",
    "- **ROUGE-1**:\n",
    "  - Precisi√≥n: 18.52% - 21.53%\n",
    "  - Recall: 36.37% - 40.98%\n",
    "  - F-Measure: 23.71% - 26.60%\n",
    "\n",
    "- **ROUGE-2**:\n",
    "  - Precisi√≥n: 4.12% - 5.42%\n",
    "  - Recall: 8.45% - 11.53%\n",
    "  - F-Measure: 5.29% - 6.87%\n",
    "\n",
    "- **ROUGE-L**:\n",
    "  - Precisi√≥n: 11.64% - 13.41%\n",
    "  - Recall: 23.40% - 27.32%\n",
    "  - F-Measure: 15.01% - 16.82%\n",
    "\n",
    "- **METEOR**:\n",
    "  - Score: 24.69%\n",
    "\n",
    "### Resultados Despu√©s del Fine-Tuning\n",
    "\n",
    "Despu√©s de aplicar el fine-tuning, los resultados fueron los siguientes:\n",
    "\n",
    "- **ROUGE-1**:\n",
    "  - Precisi√≥n: 24.79% - 28.61%\n",
    "  - Recall: 29.19% - 34.20%\n",
    "  - F-Measure: 25.83% - 29.17%\n",
    "\n",
    "- **ROUGE-2**:\n",
    "  - Precisi√≥n: 5.46% - 7.66%\n",
    "  - Recall: 6.66% - 9.90%\n",
    "  - F-Measure: 5.78% - 8.09%\n",
    "\n",
    "- **ROUGE-L**:\n",
    "  - Precisi√≥n: 15.66% - 18.18%\n",
    "  - Recall: 18.71% - 22.79%\n",
    "  - F-Measure: 16.36% - 18.77%\n",
    "\n",
    "- **METEOR**:\n",
    "  - Score: 24.69%\n",
    "\n",
    "### An√°lisis de Mejoras\n",
    "\n",
    "Comparando los resultados antes y despu√©s del fine-tuning, se observa que ha habido mejoras en todas las m√©tricas ROUGE:\n",
    "\n",
    "- **ROUGE-1** mejor√≥ en **precisi√≥n** desde 21.53% hasta 28.61%, en **recall** desde 40.98% hasta 34.20%, y en **f-measure** desde 26.60% hasta 29.17%.\n",
    "- **ROUGE-2** mostr√≥ una mejora significativa, especialmente en la **precisi√≥n**, que pas√≥ de 5.42% a 7.66%, y en la **f-measure**, que aument√≥ de 6.87% a 8.09%.\n",
    "- **ROUGE-L** tambi√©n experiment√≥ una mejora, con la **f-measure** subiendo de 16.82% a 18.77%.\n",
    "\n",
    "Sin embargo, es importante notar que el **score METEOR** se mantuvo igual en 24.69%, lo que indica que el ajuste fino no mejor√≥ esta m√©trica en particular.\n",
    "\n",
    "El score de METEOR se mantiene constante en 0.2469, tanto antes como despu√©s del fine-tuning. Esto sugiere que, aunque el modelo ha mejorado en generar palabras y secuencias similares (seg√∫n ROUGE), no ha progresado en otros aspectos como el uso de sin√≥nimos, la flexibilidad morfol√≥gica, y la fluidez global del texto generado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibertec--llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
