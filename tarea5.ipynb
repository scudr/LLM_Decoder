{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Carga del Modelo Decoder Preentrenado\n",
    "\n",
    "Cargamos el modelo GPT-2 Large, que es un modelo preentrenado de generación de texto. Este modelo es adecuado para tareas donde se necesita generar respuestas coherentes y contextualmente adecuadas a partir de una entrada textual.\n",
    "\n",
    "Los modelos GPT-2 están disponibles en varios tamaños . \n",
    "- gpt2: 110M parámetros\n",
    "- gpt2-medium: 345M parámetros\n",
    "- gpt2-large: 774M parámetros\n",
    "- gpt2-xl: 1558M parámetros\n",
    "\n",
    " Usamos la variante 'gpt2-large' para aprovechar su capacidad de generalización en textos largos y complejos. Descartamos de usar el gpt2-xl por el gran tamaño de  parámetro y recursos necesarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptación del código del proyecto para utilizar el modelo decoder en tareas de generación de texto \n",
    "El propósito de esta sección es demostrar cómo utilizar el modelo GPT-2 para la generación de texto. Se implementa una función que toma un texto de entrada, lo tokeniza y genera una secuencia de texto que sigue el contexto proporcionado. Este código es fundamental para adaptar el modelo preentrenado a tareas específicas de generación de texto, como respuestas automáticas o completación de oraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a remote place, a man named John Smith was walking along the shore of a lake when he saw a strange creature. The creature was a large, black, hairy creature with a long, thin tail. The creature was about\n"
     ]
    }
   ],
   "source": [
    "# Asignar eos_token como pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_text(input_text: str, model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, max_length: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Genera texto basado en la entrada dada utilizando un modelo de lenguaje preentrenado.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): El texto de entrada sobre el cual se generará la continuación.\n",
    "        model (GPT2LMHeadModel): El modelo seleccionado entrenado para generación de texto.\n",
    "        tokenizer (GPT2Tokenizer): El tokenizador\n",
    "        max_length (int, optional): La longitud máxima de la secuencia generada. Por defecto es 50.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto generado por el modelo.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Tokenizar la entrada y generar la máscara de atención\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Generar el texto\n",
    "    output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(generate_text(\"Once upon a time in a remote place\", model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "Para adaptar el modelo GPT-2 Large a un dominio específico, realizamos un proceso de fine-tuning utilizando el dataset PubMedQA. Este dataset contiene preguntas y respuestas del ámbito médico, lo cual es ideal para entrenar el modelo a generar respuestas coherentes y contextualmente precisas en temas de medicina. Esto permite mejorar la precisión del modelo en la generación de texto para aplicaciones específicas en este campo.\n",
    "\n",
    "**Diccionario de datos del dataset de PubMedQA**\n",
    "- pubid: Un identificador único para cada pregunta.\n",
    "- question: La pregunta que se hace sobre un tema médico.\n",
    "- context: Información contextual relacionada con la pregunta.\n",
    "- long_answer: La respuesta extensa a la pregunta.\n",
    "- final_decision: Un resumen o decisión final, que podría ser una simple respuesta afirmativa o negativa (\"yes\" o \"no\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pubid                                           question  \\\n",
      "0  21645374  Do mitochondria play a role in remodelling lac...   \n",
      "1  16418930  Landolt C and snellen e acuity: differences in...   \n",
      "2   9488747  Syncope during bathing in infants, a pediatric...   \n",
      "3  17208539  Are the long-term results of the transanal pul...   \n",
      "4  10808977  Can tailored interventions increase mammograph...   \n",
      "\n",
      "                                             context  \\\n",
      "0  {'contexts': ['Programmed cell death (PCD) is ...   \n",
      "1  {'contexts': ['Assessment of visual acuity dep...   \n",
      "2  {'contexts': ['Apparent life-threatening event...   \n",
      "3  {'contexts': ['The transanal endorectal pull-t...   \n",
      "4  {'contexts': ['Telephone counseling and tailor...   \n",
      "\n",
      "                                         long_answer final_decision  \n",
      "0  Results depicted mitochondrial dynamics in viv...            yes  \n",
      "1  Using the charts described, there was only a s...             no  \n",
      "2  \"Aquagenic maladies\" could be a pediatric form...            yes  \n",
      "3  Our long-term study showed significantly bette...             no  \n",
      "4  The effects of the intervention were most pron...            yes  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   pubid           1000 non-null   int32 \n",
      " 1   question        1000 non-null   object\n",
      " 2   context         1000 non-null   object\n",
      " 3   long_answer     1000 non-null   object\n",
      " 4   final_decision  1000 non-null   object\n",
      "dtypes: int32(1), object(4)\n",
      "memory usage: 35.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer archivo parquet con pandas\n",
    "df_labeled = pd.read_parquet('PubMedQA/pqa_labeled/train-00000-of-00001.parquet')\n",
    "\n",
    "# Mostrar las primeras filas del dataframe para ver su estructura\n",
    "print(df_labeled.head())\n",
    "\n",
    "# Mostrar información sobre el dataframe para ver las columnas y tipos de datos\n",
    "print(df_labeled.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento del dataset PubMedQA\n",
    "Se utiliza el dataset PubMedQA, que es un repositorio de preguntas y respuestas en el área de medicina. El preprocesamiento es crucial para asegurar que los datos de entrada sean consistentes y estén en el formato adecuado para el modelo. En este caso, se combinan las preguntas y contextos para formar una sola secuencia de entrada que el modelo utilizará durante el entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  Do mitochondria play a role in remodelling lac...   \n",
      "1  Landolt C and snellen e acuity: differences in...   \n",
      "2  Syncope during bathing in infants, a pediatric...   \n",
      "3  Are the long-term results of the transanal pul...   \n",
      "4  Can tailored interventions increase mammograph...   \n",
      "\n",
      "                                    summarized_input  \\\n",
      "0  Programmed cell death (PCD) is the regulated d...   \n",
      "1  100 patients (age 8 - 90 years, median 60.5 ye...   \n",
      "2  Eight infants aged 2 to 15 months were admitte...   \n",
      "3  The transanal endorectal pull-through (TERPT) ...   \n",
      "4  Can tailored interventions increase mammograph...   \n",
      "\n",
      "                                          input_text  \n",
      "0  [QUESTION] Do mitochondria play a role in remo...  \n",
      "1  [QUESTION] Landolt C and snellen e acuity: dif...  \n",
      "2  [QUESTION] Syncope during bathing in infants, ...  \n",
      "3  [QUESTION] Are the long-term results of the tr...  \n",
      "4  [QUESTION] Can tailored interventions increase...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Leer archivo parquet con pandas (si es necesario)\n",
    "# df_labeled = pd.read_parquet('PubMedQA/pqa_labeled/train-00000-of-00001.parquet')\n",
    "\n",
    "# Función para concatenar 'question' y 'context'\n",
    "def create_input_text(row):\n",
    "    question = \"[QUESTION] \" + row['question']\n",
    "    context = \"[CONTEXT] \" + \" \".join(row['context']['contexts'])\n",
    "    return question + \" \" + context\n",
    "\n",
    "# Crear la nueva columna 'input_text' combinando 'question' y 'context'\n",
    "df_labeled['input_text'] = df_labeled.apply(create_input_text, axis=1)\n",
    "\n",
    "# Pipeline para resumir texto en GPU\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "\n",
    "def summarize_input_text(text):\n",
    "    input_length = len(text.split())\n",
    "    # Ajustar max_length a un 70% de la longitud del input, pero no mayor de 150 y no menor de 20\n",
    "    max_len = min(150, max(20, int(input_length * 0.7)))\n",
    "    summarized = summarizer(text, max_length=max_len, min_length=10, do_sample=False)\n",
    "    return summarized[0]['summary_text']\n",
    "\n",
    "# Resumir la columna 'input_text'\n",
    "df_labeled['summarized_input'] = df_labeled['input_text'].apply(summarize_input_text)\n",
    "\n",
    "# Asumiendo que ya has resumido y generado la columna 'summarized_input'\n",
    "inputs = df_labeled['summarized_input'].tolist()\n",
    "outputs = df_labeled['long_answer'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar algunas de las filas generadas para verificar los resultados\n",
    "print(df_labeled[['question', 'summarized_input', 'input_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del Modelo  GPT2-Large  antes del Ajuste\n",
    "Antes de realizar el fine-tuning, es fundamental evaluar el rendimiento del modelo en su estado preentrenado. Esto nos proporciona una línea base para comparar los resultados posteriores al ajuste. Se emplean métricas como ROUGE y METEOR para medir la calidad de las respuestas generadas, lo cual es importante para tareas de generación de texto donde la coherencia y la precisión son clave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cajue\\AppData\\Local\\Temp\\ipykernel_1644\\451330542.py:43: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cajue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cajue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score before fine-tuning: {'rouge1': AggregateScore(low=Score(precision=0.18518391411503307, recall=0.3636803231088654, fmeasure=0.2371552833441397), mid=Score(precision=0.19990472388608943, recall=0.3859865089640316, fmeasure=0.2512090784722976), high=Score(precision=0.21530770974832683, recall=0.4098183511462053, fmeasure=0.2659871328262393)), 'rouge2': AggregateScore(low=Score(precision=0.04119553564750165, recall=0.0845137193138282, fmeasure=0.0529199165039611), mid=Score(precision=0.04739487692077794, recall=0.09934890898686592, fmeasure=0.060810566475866835), high=Score(precision=0.054243848071813185, recall=0.11533246988791568, fmeasure=0.0686940471216869)), 'rougeL': AggregateScore(low=Score(precision=0.11643986027572278, recall=0.23400278652838252, fmeasure=0.15008412720426026), mid=Score(precision=0.12551745050848523, recall=0.252621044122328, fmeasure=0.159853160510098), high=Score(precision=0.13412005987519693, recall=0.2732104508013579, fmeasure=0.16822604413380235)), 'rougeLsum': AggregateScore(low=Score(precision=0.12149276204620559, recall=0.24117818630920648, fmeasure=0.15607002943707649), mid=Score(precision=0.13036289579744448, recall=0.2618507131306208, fmeasure=0.16558104967245807), high=Score(precision=0.14045201905735083, recall=0.2820937434717459, fmeasure=0.1762352343442939))}\n",
      "METEOR Score before fine-tuning: {'meteor': 0.24693342198199728}\n",
      "ROUGE Score after fine-tuning: {'rouge1': AggregateScore(low=Score(precision=0.18518391411503307, recall=0.3636803231088654, fmeasure=0.2371552833441397), mid=Score(precision=0.19990472388608943, recall=0.3859865089640316, fmeasure=0.2512090784722976), high=Score(precision=0.21530770974832683, recall=0.4098183511462053, fmeasure=0.2659871328262393)), 'rouge2': AggregateScore(low=Score(precision=0.04119553564750165, recall=0.0845137193138282, fmeasure=0.0529199165039611), mid=Score(precision=0.04739487692077794, recall=0.09934890898686592, fmeasure=0.060810566475866835), high=Score(precision=0.054243848071813185, recall=0.11533246988791568, fmeasure=0.0686940471216869)), 'rougeL': AggregateScore(low=Score(precision=0.11643986027572278, recall=0.23400278652838252, fmeasure=0.15008412720426026), mid=Score(precision=0.12551745050848523, recall=0.252621044122328, fmeasure=0.159853160510098), high=Score(precision=0.13412005987519693, recall=0.2732104508013579, fmeasure=0.16822604413380235)), 'rougeLsum': AggregateScore(low=Score(precision=0.12149276204620559, recall=0.24117818630920648, fmeasure=0.15607002943707649), mid=Score(precision=0.13036289579744448, recall=0.2618507131306208, fmeasure=0.16558104967245807), high=Score(precision=0.14045201905735083, recall=0.2820937434717459, fmeasure=0.1762352343442939))}\n",
      "METEOR Score after fine-tuning: {'meteor': 0.24693342198199728}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_metric\n",
    "\n",
    "# Verificar si hay una GPU disponible y mover el modelo a la GPU si es posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Inicializar el tokenizer con padding en el lado izquierdo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\", padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizar los inputs con attention_mask\n",
    "tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "tokenized_outputs = tokenizer(outputs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Calcular la longitud máxima de entrada\n",
    "max_input_length = max([len(input_id) for input_id in tokenized_inputs['input_ids']])\n",
    "\n",
    "# Cargar el modelo preentrenado y moverlo a la GPU\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n",
    "\n",
    "# Evaluar con un conjunto de validación\n",
    "inputs_eval = tokenized_inputs['input_ids'][:100].to(device)\n",
    "attention_mask_eval = tokenized_inputs['attention_mask'][:100].to(device)\n",
    "outputs_eval = tokenized_outputs['input_ids'][:100].to(device)\n",
    "\n",
    "# Generación de texto con ajuste en max_length o max_new_tokens\n",
    "generated_outputs = model.generate(\n",
    "    inputs_eval, \n",
    "    attention_mask=attention_mask_eval,\n",
    "    max_length=max_input_length + 50,  # Utiliza la longitud máxima de entrada calculada\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Convertir las salidas a texto para la evaluación\n",
    "pred_texts_before = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs]\n",
    "ref_texts = [tokenizer.decode(ref, skip_special_tokens=True).strip().lower() for ref in outputs_eval]\n",
    "\n",
    "# Calcular ROUGE y METEOR antes del ajuste\n",
    "from datasets import load_metric\n",
    "\n",
    "# Calcular ROUGE y METEOR antes del ajuste\n",
    "rouge = load_metric(\"rouge\")\n",
    "meteor = load_metric(\"meteor\", trust_remote_code=True)\n",
    "\n",
    "# Convertir las salidas a texto para la evaluación\n",
    "pred_texts_before = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs]\n",
    "ref_texts = [tokenizer.decode(ref, skip_special_tokens=True).strip().lower() for ref in outputs_eval]\n",
    "\n",
    "# Evaluar ROUGE y METEOR antes del ajuste\n",
    "rouge_score_before = rouge.compute(predictions=pred_texts_before, references=ref_texts)\n",
    "meteor_score_before = meteor.compute(predictions=pred_texts_before, references=ref_texts)\n",
    "\n",
    "print(f\"ROUGE Score before fine-tuning: {rouge_score_before}\")\n",
    "print(f\"METEOR Score before fine-tuning: {meteor_score_before}\")\n",
    "\n",
    "\n",
    "# Generación de texto con el modelo afinado\n",
    "generated_outputs_finetuned = model.generate(\n",
    "    inputs_eval, \n",
    "    attention_mask=attention_mask_eval,\n",
    "    max_length=max_input_length + 50,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Convertir las salidas a texto para la evaluación después del ajuste\n",
    "pred_texts_after = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs_finetuned]\n",
    "\n",
    "# Evaluar ROUGE y METEOR después del ajuste\n",
    "rouge_score_after = rouge.compute(predictions=pred_texts_after, references=ref_texts)\n",
    "meteor_score_after = meteor.compute(predictions=pred_texts_after, references=ref_texts)\n",
    "\n",
    "print(f\"ROUGE Score after fine-tuning: {rouge_score_after}\")\n",
    "print(f\"METEOR Score after fine-tuning: {meteor_score_after}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning del modelo GPT-2 Large con el dataset de PubMedQA\n",
    "El fine-tuning implica entrenar el modelo preentrenado con un conjunto de datos específico para especializarlo en una tarea particular. En este caso, el modelo se entrena con el dataset PubMedQA para mejorar su capacidad de generar respuestas relacionadas con temas médicos. Este proceso de ajuste es esencial para adaptar modelos generales a aplicaciones específicas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 201/750 [00:32<01:29,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2986, 'grad_norm': 1.7591943740844727, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 401/750 [01:05<00:57,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9427, 'grad_norm': 2.4787657260894775, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 601/750 [01:38<00:25,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6776, 'grad_norm': 1.9071123600006104, 'learning_rate': 1e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [02:29<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 149.5856, 'train_samples_per_second': 20.055, 'train_steps_per_second': 5.014, 'train_loss': 2.8729229736328126, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Verificar si hay una GPU disponible y mover el modelo a la GPU si es posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cargar el tokenizer y el modelo preentrenado GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "# Asignar el token de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n",
    "\n",
    "# Definir una clase Dataset para los datos de entrenamiento\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # No necesitamos pin_memory aquí porque ya estamos en GPU\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenizar los inputs con attention_mask\n",
    "tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "tokenized_outputs = tokenizer(outputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# Preparar el dataset para el entrenamiento\n",
    "train_dataset = TextDataset(tokenized_inputs, tokenized_outputs['input_ids'])\n",
    "\n",
    "# Configurar los argumentos del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           # Carpeta de salida\n",
    "    overwrite_output_dir=True,        # Sobrescribir los resultados existentes\n",
    "    num_train_epochs=3,               # Número de épocas de entrenamiento\n",
    "    per_device_train_batch_size=4,    # Tamaño del batch de entrenamiento\n",
    "    save_steps=10_000,                # Guardar cada 10,000 pasos\n",
    "    save_total_limit=2,               # Limitar el número de checkpoints guardados\n",
    "    logging_dir='./logs',             # Directorio de los logs\n",
    "    logging_steps=200,                # Registrar cada 200 pasos\n",
    "    report_to=\"none\",                 # Desactivar reportes automáticos (e.g., a WandB)\n",
    ")\n",
    "\n",
    "# Inicializar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Comenzar el entrenamiento (fine-tuning)\n",
    "trainer.train()\n",
    "\n",
    "# Guardar el modelo afinado\n",
    "trainer.save_model(\"./fine_tuned_gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion del modelo despues del Fine-Tuning\n",
    "Después de realizar el fine-tuning, es crucial evaluar el rendimiento del modelo para asegurar que ha habido una mejora significativa. En esta sección, se utiliza el modelo ajustado para generar texto y se calculan las métricas ROUGE y METEOR para comparar los resultados antes y después del ajuste. Esta comparación permite cuantificar la mejora en la calidad de las respuestas generadas por el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score after fine-tuning: {'rouge1': AggregateScore(low=Score(precision=0.24786907670163041, recall=0.29191895230326764, fmeasure=0.25825355272835365), mid=Score(precision=0.267293526192736, recall=0.3149955497618294, fmeasure=0.2750576249441764), high=Score(precision=0.28615510480200157, recall=0.34197500281173754, fmeasure=0.2916937529305495)), 'rouge2': AggregateScore(low=Score(precision=0.05459946486472085, recall=0.06656023866251329, fmeasure=0.057800817628249276), mid=Score(precision=0.06518164797420237, recall=0.08144907164844684, fmeasure=0.06878521917562877), high=Score(precision=0.07660377025133548, recall=0.09901924636065118, fmeasure=0.0808814355654613)), 'rougeL': AggregateScore(low=Score(precision=0.15657106900324452, recall=0.187130436255338, fmeasure=0.16355496195226907), mid=Score(precision=0.1681480779597721, recall=0.20527268052793685, fmeasure=0.17524529598808058), high=Score(precision=0.18082641258680598, recall=0.22790800619432267, fmeasure=0.18764927890193514)), 'rougeLsum': AggregateScore(low=Score(precision=0.155982782152804, recall=0.18602095544912767, fmeasure=0.16375228156731203), mid=Score(precision=0.16846679942276982, recall=0.20586841101462894, fmeasure=0.1752079387821442), high=Score(precision=0.18100181004741728, recall=0.2267962154034395, fmeasure=0.18789082619067954))}\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo afinado\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\").to(device)\n",
    "\n",
    "# Generación de texto con el modelo afinado\n",
    "generated_outputs_finetuned = model.generate(\n",
    "    inputs_eval, \n",
    "    attention_mask=attention_mask_eval,\n",
    "    max_length=max_input_length + 50,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Reemplazar el cálculo de BLEU score con el cálculo de ROUGE score\n",
    "from datasets import load_metric\n",
    "\n",
    "# Calcular ROUGE Score después del fine-tuning, permitiendo código remoto\n",
    "rouge = load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "# Convertir las salidas a texto para la evaluación\n",
    "pred_texts = [tokenizer.decode(pred, skip_special_tokens=True).strip().lower() for pred in generated_outputs_finetuned]\n",
    "ref_texts = [tokenizer.decode(ref, skip_special_tokens=True).strip().lower() for ref in outputs_eval]\n",
    "\n",
    "# Calcular el ROUGE score\n",
    "rouge_scores = rouge.compute(predictions=pred_texts, references=ref_texts)\n",
    "\n",
    "print(f\"ROUGE Score after fine-tuning: {rouge_scores}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del Modelo Antes y Después del Fine-Tuning\n",
    "\n",
    "### Resultados Antes del Fine-Tuning\n",
    "\n",
    "Antes de realizar el fine-tuning, los resultados del modelo GPT-2 Large en las métricas ROUGE y METEOR fueron los siguientes:\n",
    "\n",
    "- **ROUGE-1**:\n",
    "  - Precisión: 18.52% - 21.53%\n",
    "  - Recall: 36.37% - 40.98%\n",
    "  - F-Measure: 23.71% - 26.60%\n",
    "\n",
    "- **ROUGE-2**:\n",
    "  - Precisión: 4.12% - 5.42%\n",
    "  - Recall: 8.45% - 11.53%\n",
    "  - F-Measure: 5.29% - 6.87%\n",
    "\n",
    "- **ROUGE-L**:\n",
    "  - Precisión: 11.64% - 13.41%\n",
    "  - Recall: 23.40% - 27.32%\n",
    "  - F-Measure: 15.01% - 16.82%\n",
    "\n",
    "- **METEOR**:\n",
    "  - Score: 24.69%\n",
    "\n",
    "### Resultados Después del Fine-Tuning\n",
    "\n",
    "Después de aplicar el fine-tuning, los resultados fueron los siguientes:\n",
    "\n",
    "- **ROUGE-1**:\n",
    "  - Precisión: 24.79% - 28.61%\n",
    "  - Recall: 29.19% - 34.20%\n",
    "  - F-Measure: 25.83% - 29.17%\n",
    "\n",
    "- **ROUGE-2**:\n",
    "  - Precisión: 5.46% - 7.66%\n",
    "  - Recall: 6.66% - 9.90%\n",
    "  - F-Measure: 5.78% - 8.09%\n",
    "\n",
    "- **ROUGE-L**:\n",
    "  - Precisión: 15.66% - 18.18%\n",
    "  - Recall: 18.71% - 22.79%\n",
    "  - F-Measure: 16.36% - 18.77%\n",
    "\n",
    "- **METEOR**:\n",
    "  - Score: 24.69%\n",
    "\n",
    "### Análisis de Mejoras\n",
    "\n",
    "Comparando los resultados antes y después del fine-tuning, se observa que ha habido mejoras en todas las métricas ROUGE:\n",
    "\n",
    "- **ROUGE-1** mejoró en **precisión** desde 21.53% hasta 28.61%, en **recall** desde 40.98% hasta 34.20%, y en **f-measure** desde 26.60% hasta 29.17%.\n",
    "- **ROUGE-2** mostró una mejora significativa, especialmente en la **precisión**, que pasó de 5.42% a 7.66%, y en la **f-measure**, que aumentó de 6.87% a 8.09%.\n",
    "- **ROUGE-L** también experimentó una mejora, con la **f-measure** subiendo de 16.82% a 18.77%.\n",
    "\n",
    "Sin embargo, es importante notar que el **score METEOR** se mantuvo igual en 24.69%, lo que indica que el ajuste fino no mejoró esta métrica en particular.\n",
    "\n",
    "El score de METEOR se mantiene constante en 0.2469, tanto antes como después del fine-tuning. Esto sugiere que, aunque el modelo ha mejorado en generar palabras y secuencias similares (según ROUGE), no ha progresado en otros aspectos como el uso de sinónimos, la flexibilidad morfológica, y la fluidez global del texto generado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibertec--llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
